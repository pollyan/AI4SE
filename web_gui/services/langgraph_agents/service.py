"""
LangGraph æ™ºèƒ½ä½“æœåŠ¡å°è£…

æä¾›ç»Ÿä¸€çš„æœåŠ¡æ¥å£ï¼Œæ”¯æŒï¼š
- å¤šç§åŠ©æ‰‹ç±»å‹ï¼ˆAlexã€Lisa ç­‰ï¼‰
- æµå¼å’Œéæµå¼å¯¹è¯
- PostgreSQL æŒä¹…åŒ–æ£€æŸ¥ç‚¹
- LangSmith è¿½è¸ªå’Œè§‚æµ‹ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡è‡ªåŠ¨å¯ç”¨ï¼‰
"""

import os
import logging
from typing import AsyncIterator, Optional, Dict, Any
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver

from .graph import get_graph_for_assistant
from .state import AssistantState

logger = logging.getLogger(__name__)


class LangGraphAssistantService:
    """
    LangGraph æ™ºèƒ½ä½“æœåŠ¡
    
    æä¾›åŸºäº LangGraph çš„æ™ºèƒ½åŠ©æ‰‹åŠŸèƒ½ï¼Œæ”¯æŒæµå¼å¯¹è¯å’Œæ£€æŸ¥ç‚¹æŒä¹…åŒ–ã€‚
    """
    
    def __init__(self, assistant_type: str, database_url: Optional[str] = None, use_checkpointer: bool = True):
        """
        åˆå§‹åŒ– LangGraph æœåŠ¡
        
        Args:
            assistant_type: åŠ©æ‰‹ç±»å‹ ('alex' æˆ– 'lisa')
            database_url: PostgreSQL æ•°æ®åº“è¿æ¥ URL (å¯é€‰)
            use_checkpointer: æ˜¯å¦ä½¿ç”¨æ£€æŸ¥ç‚¹æŒä¹…åŒ– (é»˜è®¤ True)
        """
        self.assistant_type = assistant_type
        self.database_url = database_url
        self.use_checkpointer = use_checkpointer
        self.session_id = None
        self.graph = None
        self.checkpointer = None
        
        logger.info(f"åˆå§‹åŒ– LangGraph æ™ºèƒ½ä½“æœåŠ¡: {assistant_type}")
        
        # LangSmith è¿½è¸ªï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡è‡ªåŠ¨å¯ç”¨ï¼Œæ— éœ€é¢å¤–ä»£ç ï¼‰
        if os.getenv('LANGCHAIN_TRACING_V2') == 'true':
            langchain_project = os.getenv('LANGCHAIN_PROJECT', 'intent-test-framework')
            logger.info(f"âœ… LangSmith è¿½è¸ªå·²å¯ç”¨")
            logger.info(f"   é¡¹ç›®: {langchain_project}")
            logger.info(f"   æ¨¡å¼: è‡ªåŠ¨è¿½è¸ªï¼ˆLangChain å†…ç½®ï¼‰")
        else:
            logger.info("â„¹ï¸ LangSmith è¿½è¸ªæœªå¯ç”¨ï¼ˆè®¾ç½® LANGCHAIN_TRACING_V2=true å¯ç”¨ï¼‰")
    
    async def _setup_checkpointer(self):
        """
        è®¾ç½® PostgreSQL æ£€æŸ¥ç‚¹ä¿å­˜å™¨
        
        ä½¿ç”¨ç°æœ‰æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ã€‚
        """
        if not self.use_checkpointer:
            logger.info("è·³è¿‡æ£€æŸ¥ç‚¹è®¾ç½®ï¼ˆuse_checkpointer=Falseï¼‰")
            return None
        
        try:
            # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è·å–æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
            database_url = os.getenv("DATABASE_URL")
            
            if not database_url:
                logger.warning("æœªæ‰¾åˆ° DATABASE_URLï¼Œä½¿ç”¨å†…å­˜æ¨¡å¼")
                return None
            
            # åˆ›å»ºå¼‚æ­¥ PostgreSQL æ£€æŸ¥ç‚¹ä¿å­˜å™¨
            checkpointer = AsyncPostgresSaver.from_conn_string(database_url)
            
            # åˆå§‹åŒ–æ£€æŸ¥ç‚¹è¡¨
            await checkpointer.setup()
            
            logger.info("PostgreSQL æ£€æŸ¥ç‚¹ä¿å­˜å™¨è®¾ç½®å®Œæˆ")
            return checkpointer
            
        except Exception as e:
            logger.error(f"è®¾ç½®æ£€æŸ¥ç‚¹ä¿å­˜å™¨å¤±è´¥: {str(e)}")
            logger.warning("é™çº§ä¸ºå†…å­˜æ¨¡å¼ï¼ˆæ— çŠ¶æ€æŒä¹…åŒ–ï¼‰")
            return None
    
    async def initialize(self):
        """
        å¼‚æ­¥åˆå§‹åŒ–æœåŠ¡
        
        å¿…é¡»åœ¨ä½¿ç”¨æœåŠ¡å‰è°ƒç”¨ã€‚
        """
        logger.info(f"å¼‚æ­¥åˆå§‹åŒ– {self.assistant_type} æ™ºèƒ½ä½“æœåŠ¡")
        
        # è®¾ç½®æ£€æŸ¥ç‚¹
        self.checkpointer = await self._setup_checkpointer()
        
        # åˆ›å»ºå›¾
        self.graph = get_graph_for_assistant(
            self.assistant_type,
            checkpointer=self.checkpointer
        )
        
        logger.info(f"{self.assistant_type} æ™ºèƒ½ä½“æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    async def process_message(
        self, 
        session_id: str, 
        user_message: str,
        project_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼ˆéæµå¼ï¼‰
        
        Args:
            session_id: ä¼šè¯ IDï¼ˆç”¨ä½œ thread_idï¼‰
            user_message: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            project_name: å¯é€‰çš„é¡¹ç›®åç§°
            
        Returns:
            åŒ…å« AI å›å¤çš„å­—å…¸
        """
        if not self.graph:
            await self.initialize()
        
        logger.info(f"å¤„ç†æ¶ˆæ¯ - ä¼šè¯: {session_id}, æ¶ˆæ¯é•¿åº¦: {len(user_message)}")
        
        try:
            # é…ç½®ï¼ˆä½¿ç”¨ session_id ä½œä¸º thread_idï¼‰
            # LangSmith ä¼šè‡ªåŠ¨è®°å½•æ‰€æœ‰ LangChain/LangGraph è°ƒç”¨
            config = {
                "configurable": {
                    "thread_id": session_id
                },
                "tags": [self.assistant_type, "langgraph"],
                "metadata": {
                    "session_id": session_id,
                    "project_name": project_name or "default"
                }
            }
            
            # æ„å»ºè¾“å…¥
            input_data = {
                "messages": [HumanMessage(content=user_message)],
                "session_id": session_id,
                "assistant_type": self.assistant_type,
                "project_name": project_name,
            }
            
            # è°ƒç”¨å›¾
            result = await self.graph.ainvoke(input_data, config=config)
            
            # æå– AI å›å¤
            messages = result.get("messages", [])
            ai_message = None
            for msg in reversed(messages):
                if isinstance(msg, AIMessage):
                    ai_message = msg.content
                    break
            
            if not ai_message:
                raise ValueError("æœªèƒ½ä»å›¾ä¸­è·å– AI å›å¤")
            
            logger.info(f"æ¶ˆæ¯å¤„ç†å®Œæˆï¼Œå›å¤é•¿åº¦: {len(ai_message)}")
            
            return {
                "ai_response": ai_message,
                "consensus_content": result.get("consensus_content", {}),
                "analysis_context": result.get("analysis_context", {}),
                "current_stage": result.get("current_stage", "initial"),
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯å¤±è´¥: {str(e)}")
            raise
    
    async def stream_message(
        self, 
        session_id: str, 
        user_message: str,
        project_name: Optional[str] = None,
        is_activated: bool = False  # âœ¨ æ¿€æ´»çŠ¶æ€æ ‡å¿—
    ) -> AsyncIterator[str]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼ˆæµå¼å“åº”ï¼‰
        
        Args:
            session_id: ä¼šè¯ ID
            user_message: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            project_name: å¯é€‰çš„é¡¹ç›®åç§°
            is_activated: ä¼šè¯æ˜¯å¦å·²æ¿€æ´»ï¼ˆå½±å“æ¬¢è¿æ¶ˆæ¯æ˜¾ç¤ºï¼‰
            
        Yields:
            AI å›å¤çš„æ–‡æœ¬ç‰‡æ®µ
        """
        if not self.graph:
            await self.initialize()
        
        logger.info(f"æµå¼å¤„ç†æ¶ˆæ¯ - ä¼šè¯: {session_id}, æ¶ˆæ¯é•¿åº¦: {len(user_message)}")
        logger.info(f"ç”¨æˆ·æ¶ˆæ¯é¢„è§ˆ: {user_message[:200]}...")
        
        try:
            # é…ç½®ï¼ˆä½¿ç”¨ session_id ä½œä¸º thread_idï¼‰
            # LangSmith ä¼šè‡ªåŠ¨è®°å½•æ‰€æœ‰ LangChain/LangGraph è°ƒç”¨
            config = {
                "configurable": {
                    "thread_id": session_id
                },
                "tags": [self.assistant_type, "langgraph"],
                "metadata": {
                    "session_id": session_id,
                    "project_name": project_name or "default"
                }
            }
            
            #æ„å»ºè¾“å…¥
            input_data = {
                "messages": [HumanMessage(content=user_message)],
                "session_id": session_id,
                "assistant_type": self.assistant_type,
                "project_name": project_name,
                "is_activated": is_activated,  # âœ¨ ä¼ é€’æ¿€æ´»çŠ¶æ€
            }
            
            # ğŸš¨ ä¸´æ—¶è°ƒè¯•ï¼šç»•è¿‡å›¾ï¼Œç›´æ¥è¿”å›å›ºå®šå“åº”
            BYPASS_GRAPH = False  # è®¾ä¸º False æ¢å¤æ­£å¸¸
            
            if BYPASS_GRAPH and self.assistant_type == "lisa_v2":
                logger.warning("ğŸš¨ ä½¿ç”¨è°ƒè¯•ç»•è¿‡æ¨¡å¼")
                test_response = """æ‚¨å¥½ï¼æˆ‘æ˜¯ **Lisa Song**ï¼Œæ‚¨çš„é¦–å¸­æµ‹è¯•é¢†åŸŸä¸“å®¶ã€‚

**ã€è°ƒè¯•æ¨¡å¼ã€‘** è¿™æ˜¯ä¸€ä¸ªå›ºå®šçš„æµ‹è¯•å“åº”ï¼Œç”¨äºæ’æŸ¥é—®é¢˜ã€‚

è¯·å‘Šè¯‰æˆ‘æ‚¨çš„æµ‹è¯•éœ€æ±‚ã€‚"""
                
                # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                for char in test_response:
                    yield char
                    await asyncio.sleep(0.01)
                return
            
            # æµå¼è°ƒç”¨å›¾
            has_streamed_content = False
            async for event in self.graph.astream_events(input_data, config=config, version="v2"):
                # æ–¹æ³•1: æå– LLM çš„æµå¼è¾“å‡ºï¼ˆé€å­—ç¬¦ï¼‰
                if event["event"] == "on_chat_model_stream":
                    chunk = event["data"]["chunk"]
                    if hasattr(chunk, "content") and chunk.content:
                        has_streamed_content = True
                        yield chunk.content
                
                # æ–¹æ³•2: æ•è·èŠ‚ç‚¹å®Œæˆäº‹ä»¶ï¼ˆå¤„ç†é™æ€æ¶ˆæ¯ï¼Œå¦‚ Lisa çš„æ¬¢è¿æ¶ˆæ¯ï¼‰
                elif event["event"] == "on_chain_end" and not has_streamed_content:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ AI æ¶ˆæ¯è¾“å‡º
                    output = event.get("data", {}).get("output", {})
                    if isinstance(output, dict) and "messages" in output:
                        messages = output["messages"]
                        # æŸ¥æ‰¾æœ€æ–°çš„ AI æ¶ˆæ¯
                        for msg in reversed(messages):
                            if isinstance(msg, AIMessage) and msg.content:
                                # âœ¨ å°†é™æ€æ¶ˆæ¯åˆ†å—æµå¼è¿”å›ï¼Œæ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                                logger.info(f"æ£€æµ‹åˆ°é™æ€ AI æ¶ˆæ¯ï¼Œé•¿åº¦: {len(msg.content)}ï¼Œå¼€å§‹åˆ†å—æµå¼è¾“å‡º")
                                
                                import asyncio
                                # æ¯æ¬¡å‘é€ 10-15 ä¸ªå­—ç¬¦
                                chunk_size = 12
                                content = msg.content
                                for i in range(0, len(content), chunk_size):
                                    chunk = content[i:i + chunk_size]
                                    yield chunk
                                    # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿæ‰“å­—é€Ÿåº¦ï¼ˆ30msï¼‰
                                    await asyncio.sleep(0.03)
                                
                                has_streamed_content = True
                                break
            
            logger.info("æµå¼æ¶ˆæ¯å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æµå¼å¤„ç†æ¶ˆæ¯å¤±è´¥: {str(e)}")
            yield f"\n\né”™è¯¯: {str(e)}"
    
    async def get_session_history(
        self, 
        session_id: str
    ) -> Dict[str, Any]:
        """
        è·å–ä¼šè¯å†å²
        
        Args:
            session_id: ä¼šè¯ ID
            
        Returns:
            ä¼šè¯çŠ¶æ€å­—å…¸
        """
        if not self.graph or not self.checkpointer:
            logger.warning("æ— æ³•è·å–å†å²ï¼šå›¾æˆ–æ£€æŸ¥ç‚¹æœªåˆå§‹åŒ–")
            return {}
        
        try:
            config = {
                "configurable": {
                    "thread_id": session_id
                }
            }
            
            # è·å–æœ€æ–°çŠ¶æ€
            state = await self.graph.aget_state(config)
            
            return state.values if state else {}
            
        except Exception as e:
            logger.error(f"è·å–ä¼šè¯å†å²å¤±è´¥: {str(e)}")
            return {}


# ä¸ºäº†ä¿æŒå…¼å®¹ï¼Œåˆ›å»ºåˆ«å
RequirementsAIService = LangGraphAssistantService
